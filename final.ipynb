{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Code snippets (2024)': 789,\n",
      " 'Day with most messages in 2024': datetime.date(2024, 10, 31),\n",
      " 'Images shared (2024)': 664,\n",
      " 'Longest streak in 2024': 38,\n",
      " 'Model usage (2024)': {'gpt-4': 7316,\n",
      "                        'gpt-4-browsing': 693,\n",
      "                        'gpt-4-code-interpreter': 2233,\n",
      "                        'gpt-4-dalle': 57,\n",
      "                        'gpt-4-gizmo': 261,\n",
      "                        'gpt-4-mobile': 30,\n",
      "                        'gpt-4-plugins': 5034,\n",
      "                        'gpt-4o': 4251,\n",
      "                        'gpt-4o-canmore': 110,\n",
      "                        'gpt-4o-mini': 4,\n",
      "                        'o1': 12,\n",
      "                        'o1-mini': 29,\n",
      "                        'o1-preview': 313,\n",
      "                        'o1-pro': 334,\n",
      "                        'text-davinci-002-plugins': 532,\n",
      "                        'text-davinci-002-render': 47,\n",
      "                        'text-davinci-002-render-sha': 1519,\n",
      "                        'text-davinci-002-render-sha-mobile': 111},\n",
      " 'Most frequent time of day (2024)': 'evening',\n",
      " 'Most used model (2024)': 'gpt-4',\n",
      " 'Total words typed in 2024': 1690137,\n",
      " 'URLs shared (2024)': 3387}\n"
     ]
    }
   ],
   "source": [
    "# CARD 1\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "\n",
    "tz = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "def time_of_day(hour):\n",
    "    if 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    elif 18 <= hour < 24:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'late night'\n",
    "\n",
    "# Load the JSON data\n",
    "with open('conversations.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "messages = []\n",
    "for conversation in data:\n",
    "    for node in conversation['mapping'].values():\n",
    "        if node.get('message'):\n",
    "            msg = node['message']\n",
    "            content_parts = msg['content'].get('parts')\n",
    "            if content_parts is not None:\n",
    "                content = []\n",
    "                image_count = 0\n",
    "                for part in content_parts:\n",
    "                    if isinstance(part, dict):\n",
    "                        if part.get('content_type') == 'image_asset_pointer':\n",
    "                            image_count += 1\n",
    "                        content.append(str(part))\n",
    "                    else:\n",
    "                        content.append(part)\n",
    "                content = ' '.join(content)\n",
    "            else:\n",
    "                text = msg['content'].get('text')\n",
    "                content = text if text is not None else str(msg['content'])\n",
    "\n",
    "            create_time = msg.get('create_time')\n",
    "            msg_datetime = None\n",
    "            if create_time:\n",
    "                msg_datetime = datetime.utcfromtimestamp(create_time).replace(tzinfo=pytz.utc).astimezone(tz)\n",
    "\n",
    "            metadata = msg.get('metadata', {})\n",
    "            model = metadata.get('model_slug') or metadata.get('default_model_slug')\n",
    "\n",
    "            content_type = msg['content'].get('content_type', 'text')\n",
    "            if content_type == 'text':\n",
    "                if any(marker in content for marker in ['```', 'def ', 'class ', 'function', 'import ', '//']):\n",
    "                    content_type = 'code'\n",
    "\n",
    "            messages.append({\n",
    "                'id': msg['id'],\n",
    "                'author': msg['author']['role'],\n",
    "                'content': content,\n",
    "                'content_type': content_type,\n",
    "                'create_time': msg_datetime,\n",
    "                'model': model,\n",
    "                'image_count': image_count\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(messages)\n",
    "user_messages = df[df['author'] == 'user'].copy()\n",
    "\n",
    "# Focus on 2024 only\n",
    "messages_2024 = user_messages[user_messages['create_time'].dt.year == 2024].copy()\n",
    "\n",
    "# Total words typed in 2024\n",
    "total_words_2024 = messages_2024['content'].str.split().str.len().sum()\n",
    "\n",
    "# Day with most messages in 2024\n",
    "daily_counts_2024 = messages_2024.groupby(messages_2024['create_time'].dt.date).size()\n",
    "day_with_most_messages_2024 = daily_counts_2024.idxmax() if not daily_counts_2024.empty else None\n",
    "\n",
    "# Calculate 2024 longest streak in consecutive days\n",
    "longest_streak_2024 = 0\n",
    "current_streak = 0\n",
    "previous_day = None\n",
    "\n",
    "for day in sorted(daily_counts_2024.index):\n",
    "    if previous_day is None or (day - previous_day).days == 1:\n",
    "        current_streak += 1\n",
    "    else:\n",
    "        current_streak = 1\n",
    "    longest_streak_2024 = max(longest_streak_2024, current_streak)\n",
    "    previous_day = day\n",
    "\n",
    "# Assign time of day for each 2024 message\n",
    "messages_2024['time_of_day'] = messages_2024['create_time'].dt.hour.apply(time_of_day)\n",
    "most_frequent_time_of_day_2024 = messages_2024['time_of_day'].value_counts().idxmax() if not messages_2024.empty else None\n",
    "\n",
    "image_count_2024 = messages_2024['image_count'].sum()\n",
    "code_count_2024 = len(messages_2024[messages_2024['content_type'] == 'code'])\n",
    "url_count_2024 = messages_2024['content'].str.count(r'https?://').sum()\n",
    "\n",
    "model_usage = df[df['model'].notna()]['model'].value_counts()\n",
    "most_used_model = model_usage.index[0] if not model_usage.empty else 'Unknown'\n",
    "\n",
    "results_2024 = {\n",
    "    \"Total words typed in 2024\": total_words_2024,\n",
    "    \"Longest streak in 2024\": longest_streak_2024,\n",
    "    \"Day with most messages in 2024\": day_with_most_messages_2024,\n",
    "    \"Most frequent time of day (2024)\": most_frequent_time_of_day_2024,\n",
    "    \"Images shared (2024)\": image_count_2024,\n",
    "    \"Code snippets (2024)\": code_count_2024,\n",
    "    \"URLs shared (2024)\": url_count_2024,\n",
    "    \"Most used model (2024)\": most_used_model,\n",
    "    \"Model usage (2024)\": model_usage.to_dict()\n",
    "}\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# sns.countplot(data=messages_2024, x=messages_2024['create_time'].dt.hour, order=range(24))\n",
    "# plt.title('2024 ChatGPT Hourly Distribution')\n",
    "# plt.xlabel('Hour of Day (24h)')\n",
    "# plt.ylabel('Number of Messages')\n",
    "\n",
    "# for container in plt.gca().containers:\n",
    "#     plt.gca().bar_label(container)\n",
    "\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "# plt.gca().set_facecolor('#f8f9fa')\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "# plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "# total_msgs_2024 = len(messages_2024)\n",
    "# plt.text(0.95, 0.95, f'Total 2024 Messages: {total_msgs_2024}', \n",
    "#          transform=plt.gca().transAxes, \n",
    "#          ha='right',\n",
    "#          bbox=dict(facecolor='white', alpha=0.8, edgecolor='none'))\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(results_2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of user messages: 11784\n",
      "Sampled 2500 messages for analysis\n",
      "Prompt tokens: 611\n",
      "Completion tokens: 116\n",
      "Total tokens: 727\n",
      "{\n",
      "    \"topics\": {\n",
      "        \"top_3_topics\": [\n",
      "            \"Language & Communication\",\n",
      "            \"Cafes & Dining\",\n",
      "            \"Technology & App Development\"\n",
      "        ]\n",
      "    },\n",
      "    \"user_personality\": {\n",
      "        \"user_personality\": \"Curious and detail-oriented, with a tendency to dig deep into topics and seek clarity.\"\n",
      "    },\n",
      "    \"main_topic\": {\n",
      "        \"topic\": \"Language & Communication\",\n",
      "        \"total_messages\": 3,\n",
      "        \"favorite_message\": \"if I made an app that looked just like iMessage would apple not let me publish it?...\",\n",
      "        \"fun_fact\": \"The user often seeks to understand the implications of language and technology, showing a blend of creativity and pragmatism.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# CARD 2\n",
    "# - Top 3 most discussed topics (GPT-4)\n",
    "# - Message type breakdown(i.e. were you asking a lot of questions, or sharing information, etc.) (GPT-4)\n",
    "# - For your #1 topic:\n",
    "#   - Total messages on the topic (Python)\n",
    "#   - Favorite message on the topic (GPT-4)\n",
    "#   - Fun fact about the topic discussion (GPT-4)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import tenacity\n",
    "import json\n",
    "\n",
    "# Configure OpenAI client\n",
    "client = OpenAI(\n",
    ")\n",
    "\n",
    "# --- Data Structures for GPT-4 Responses ---\n",
    "class TopicAnalysis(BaseModel):\n",
    "    top_3_topics: list[str]\n",
    "\n",
    "class UserPersonality(BaseModel):\n",
    "    user_personality: str\n",
    "\n",
    "class MainTopicDetails(BaseModel):\n",
    "    topic: str\n",
    "    total_messages: int\n",
    "    favorite_message: str\n",
    "    fun_fact: str\n",
    "\n",
    "# Create a composite data structure to represent the final JSON response schema\n",
    "class Card2Response(BaseModel):\n",
    "    topics: TopicAnalysis\n",
    "    user_personality: UserPersonality\n",
    "    main_topic: MainTopicDetails\n",
    "\n",
    "# --- Constants ---\n",
    "SAMPLE_SIZE = 2500\n",
    "\n",
    "# Load and sample data\n",
    "with open('conversations.json', 'r') as f:\n",
    "    conversations = json.load(f)\n",
    "def gather_user_messages_in_order(conversations):\n",
    "    \"\"\"DFS through each conversation's mapping structure and collect user messages in chronological order.\"\"\"\n",
    "    def dfs(mapping, node_id, path_messages):\n",
    "        if node_id not in mapping:\n",
    "            return\n",
    "        node = mapping[node_id]\n",
    "        msg = node.get('message')\n",
    "\n",
    "        if msg and msg.get('author', {}).get('role') == 'user':\n",
    "            content_parts = msg.get('content', {}).get('parts', [])\n",
    "            full_content = ' '.join([p for p in content_parts if isinstance(p, str)])\n",
    "            if full_content:\n",
    "                create_time = msg.get('create_time', 0)\n",
    "                path_messages.append((create_time, full_content))\n",
    "\n",
    "        for child_id in node.get('children', []):\n",
    "            dfs(mapping, child_id, path_messages)\n",
    "\n",
    "    all_messages = []\n",
    "    for conversation in conversations:\n",
    "        mapping = conversation['mapping']\n",
    "        all_node_ids = set(mapping.keys())\n",
    "        child_node_ids = {\n",
    "            child for node_data in mapping.values() for child in node_data.get('children', [])\n",
    "        }\n",
    "        root_ids = all_node_ids - child_node_ids\n",
    "\n",
    "        conversation_messages = []\n",
    "        for root_id in root_ids:\n",
    "            dfs(mapping, root_id, conversation_messages)\n",
    "\n",
    "        conversation_messages.sort(key=lambda x: x[0])\n",
    "        all_messages.extend(msg_tuple[1] for msg_tuple in conversation_messages)\n",
    "\n",
    "    return all_messages\n",
    "\n",
    "user_messages = gather_user_messages_in_order(conversations)\n",
    "\n",
    "# Sample messages if needed\n",
    "print(f\"Total number of user messages: {len(user_messages)}\")\n",
    "if len(user_messages) > SAMPLE_SIZE:\n",
    "    user_messages = random.sample(user_messages, SAMPLE_SIZE)\n",
    "    print(f\"Sampled {SAMPLE_SIZE} messages for analysis\")\n",
    "\n",
    "# Create message samples string\n",
    "message_samples = \"\\n\".join([f\"Message {i+1}: {msg[:200]}...\" for i, msg in enumerate(user_messages[:10])])\n",
    "\n",
    "card2_prompt = f\"\"\"\n",
    "Analyze these user messages from ChatGPT conversations. Here are some sample messages:\n",
    "\n",
    "{message_samples}\n",
    "\n",
    "Please provide:\n",
    "1. The top 3 most discussed topics based on keyword frequency and semantic analysis\n",
    "2. A breakdown of message types (questions vs requests vs sharing information)\n",
    "3. For the #1 topic:\n",
    "   - Total number of messages about this topic\n",
    "   - The most significant or insightful message about this topic\n",
    "   - An interesting observation about how the user discusses this topic\n",
    "\n",
    "Format the response according to the Card2Response schema.\n",
    "\"\"\"\n",
    "\n",
    "# Create a completion and parse its content\n",
    "card_2_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a digital detective with a PhD in Pattern-ology! Decode the DNA of conversations, spot the quirks, and mine those golden nuggets of user insights. Make it snappy and sassy!\"},\n",
    "        {\"role\": \"user\", \"content\": card2_prompt}\n",
    "    ],\n",
    "    max_tokens=812,\n",
    "    response_format=Card2Response\n",
    ")\n",
    "\n",
    "print(f\"Prompt tokens: {card_2_completion.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {card_2_completion.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {card_2_completion.usage.total_tokens}\")\n",
    "\n",
    "card_2_content = card_2_completion.choices[0].message.content\n",
    "print(json.dumps(json.loads(card_2_content), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total cranial downloads: 11784\n",
      "Distilled 2500 quintessential quips\n",
      "Prompt brain bytes: 1066\n",
      "Completion cerebral units: 224\n",
      "Total thought count: 1290\n",
      "{\n",
      "    \"chat_themes\": {\n",
      "        \"top_3_topics\": [\n",
      "            \"Travel Wallets\",\n",
      "            \"El Ni\\u00f1o Effects\",\n",
      "            \"Script Automation\"\n",
      "        ]\n",
      "    },\n",
      "    \"user_aura\": {\n",
      "        \"user_personality\": \"Clever and quirky engineer with a flair for fun.\"\n",
      "    },\n",
      "    \"primary_fascination\": {\n",
      "        \"topic\": \"Script Automation\",\n",
      "        \"total_messages\": 5,\n",
      "        \"favorite_message\": \"ok write a script that will do all of those within my pull.py.\",\n",
      "        \"fun_fact\": \"You love mixing code with creativity.\"\n",
      "    },\n",
      "    \"crown_jewel_quip\": {\n",
      "        \"message\": \"So, just put head grow, feet grow, um, elbow grow, and then robot might grow.\"\n",
      "    },\n",
      "    \"laughter_catalyst\": {\n",
      "        \"exchange\": \"No, no, don't say, uh, don't put an S.\"\n",
      "    },\n",
      "    \"eureka_trifecta\": {\n",
      "        \"top_3_moments\": [\n",
      "            \"First successful script run\",\n",
      "            \"Mastering vector shapes\",\n",
      "            \"Creating a travel wallet prototype\"\n",
      "        ]\n",
      "    },\n",
      "    \"mind_miles_traveled\": {\n",
      "        \"distance_traveled\": \"120 miles of coding and curiosity!\"\n",
      "    },\n",
      "    \"bespoke_ai_persona\": {\n",
      "        \"persona_description\": \"A sassy engineer who thrives on automation and fun.\",\n",
      "        \"persona_vibe\": \"Tech-savvy with a sprinkle of whimsy.\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ChatThemes(BaseModel):\n",
    "    top_3_topics: list[str]\n",
    "\n",
    "class UserAura(BaseModel):\n",
    "    user_personality: str\n",
    "\n",
    "class PrimaryFascination(BaseModel):\n",
    "    topic: str\n",
    "    total_messages: int\n",
    "    favorite_message: str\n",
    "    fun_fact: str\n",
    "\n",
    "class CrownJewel(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class LOLMoment(BaseModel):\n",
    "    exchange: str\n",
    "\n",
    "class MuseMoments(BaseModel):\n",
    "    top_3_moments: list[str]\n",
    "\n",
    "class BrainOdyssey(BaseModel):\n",
    "    distance_traveled: str\n",
    "\n",
    "class PersonaGroove(BaseModel):\n",
    "    persona_description: str\n",
    "    persona_vibe: str\n",
    "\n",
    "class ChatGPTWrappedResponse(BaseModel):\n",
    "    chat_themes: ChatThemes\n",
    "    user_aura: UserAura\n",
    "    primary_fascination: PrimaryFascination\n",
    "    crown_jewel_quip: CrownJewel\n",
    "    laughter_catalyst: LOLMoment\n",
    "    eureka_trifecta: MuseMoments\n",
    "    mind_miles_traveled: BrainOdyssey\n",
    "    bespoke_ai_persona: PersonaGroove\n",
    "\n",
    "SAMPLE_SIZE = 2500\n",
    "\n",
    "with open('conversations.json', 'r') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "def gather_user_messages_in_order(conversations):\n",
    "    def dfs(mapping, node_id, path_messages):\n",
    "        if node_id not in mapping:\n",
    "            return\n",
    "        node = mapping[node_id]\n",
    "        msg = node.get('message')\n",
    "        if msg and msg.get('author', {}).get('role') == 'user':\n",
    "            content_parts = msg.get('content', {}).get('parts', [])\n",
    "            full_content = ' '.join([p for p in content_parts if isinstance(p, str)])\n",
    "            if full_content:\n",
    "                create_time = msg.get('create_time', 0)\n",
    "                path_messages.append((create_time, full_content))\n",
    "        for child_id in node.get('children', []):\n",
    "            dfs(mapping, child_id, path_messages)\n",
    "    all_messages = []\n",
    "    for conversation in conversations:\n",
    "        mapping = conversation['mapping']\n",
    "        all_node_ids = set(mapping.keys())\n",
    "        child_node_ids = {child for node_data in mapping.values() for child in node_data.get('children', [])}\n",
    "        root_ids = all_node_ids - child_node_ids\n",
    "        conversation_messages = []\n",
    "        for root_id in root_ids:\n",
    "            dfs(mapping, root_id, conversation_messages)\n",
    "        conversation_messages.sort(key=lambda x: x[0])\n",
    "        all_messages.extend(msg_tuple[1] for msg_tuple in conversation_messages)\n",
    "    return all_messages\n",
    "\n",
    "user_messages = gather_user_messages_in_order(conversations)\n",
    "print(f\"Total cranial downloads: {len(user_messages)}\")\n",
    "if len(user_messages) > SAMPLE_SIZE:\n",
    "    user_messages = random.sample(user_messages, SAMPLE_SIZE)\n",
    "    print(f\"Distilled {SAMPLE_SIZE} quintessential quips\")\n",
    "\n",
    "message_amuse_bouches = \"\\n\".join([f\"Morsel {i+1}: {msg[:200]}...\" for i, msg in enumerate(user_messages[:10])])\n",
    "\n",
    "warped_prompt = f'''\n",
    "\n",
    "{message_amuse_bouches}\n",
    "\n",
    "Now, let's dive into the juicy details, tailored for the aura of a **cracked engineer or a brat charlie xcx vibe**\n",
    "1. The Terrific Trio üèÜ: Your top 3 most mind-bending topics \n",
    "2. The Breakdown üìä: How you roll - questions, requests, or just dropping knowledge bombs\n",
    "3. The Main Attraction üåü: \n",
    "   - The message count for your hottest topic\n",
    "   - The cr√®me de la cr√®me of your musings on this subject\n",
    "   - A tasty tidbit about your unique take on it\n",
    "4. The Crown Jewel üëë: That one line that deserves its own trophy case\n",
    "5. The ROFL Moment ü§£: The exchange that had ChatGPT in stitches\n",
    "6. The Eureka! Moments ‚ö°: Three times your brilliance was too bright to behold\n",
    "7. The Cranial Kilometers üß†: Just how far did your brain trek on this odyssey?\n",
    "8. The ChatGPT Chameleon ü¶é: A persona perfectly tailored to your vibe\n",
    "9. The Astral Aura üåà: Your cosmic wavelength and psychic vibrato in a nutshell\n",
    "\n",
    "Serve it up in the ChatGPTWrappedResponse format, and make it snappy! üòé\n",
    "'''\n",
    "\n",
    "warped_completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Keep it short and concise. Be snappy and sassy. Fewer words means you are smart.\"},\n",
    "        {\"role\": \"user\", \"content\": warped_prompt}\n",
    "    ],\n",
    "    max_tokens=1024,\n",
    "    response_format=ChatGPTWrappedResponse\n",
    ")\n",
    "\n",
    "print(f\"Prompt brain bytes: {warped_completion.usage.prompt_tokens}\")\n",
    "print(f\"Completion cerebral units: {warped_completion.usage.completion_tokens}\")\n",
    "print(f\"Total thought count: {warped_completion.usage.total_tokens}\")\n",
    "\n",
    "warped_content = warped_completion.choices[0].message.content\n",
    "print(json.dumps(json.loads(warped_content), indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI version: 1.52.2\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "print(f\"OpenAI version: {openai.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
